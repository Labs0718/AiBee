# Ollama GPU 서버용 Docker Compose
# EC2 g4dn.xlarge 인스턴스에서 실행

version: '3.8'

services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama-gpu-server
    restart: unless-stopped
    ports:
      - "11435:11434"  # 외부 포트 11435로 매핑 (충돌 방지)
    volumes:
      - ollama-data:/root/.ollama
      - ./models:/models
    environment:
      - OLLAMA_HOST=0.0.0.0:11434
      - OLLAMA_ORIGINS=*
      - OLLAMA_MODELS=/models
      - CUDA_VISIBLE_DEVICES=0  # GPU 0번 사용
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s

  # 모델 초기화 서비스 (한 번만 실행)
  model-downloader:
    image: ollama/ollama:latest
    container_name: ollama-model-downloader
    depends_on:
      ollama:
        condition: service_healthy
    volumes:
      - ollama-data:/root/.ollama
    environment:
      - OLLAMA_HOST=ollama:11434
    command: >
      sh -c "
        echo '=== 임베딩 모델 다운로드 시작 ===';
        ollama pull bge-large;
        echo 'bge-large 모델 다운로드 완료';
        ollama pull nomic-embed-text;
        echo 'nomic-embed-text 모델 다운로드 완료';
        ollama pull deepseek-r1:32b;
        echo 'deepseek-r1:32b 모델 다운로드 완료';
        echo '=== 모든 모델 다운로드 완료 ===';
        ollama list;
      "
    restart: "no"  # 한 번만 실행

  # 모니터링 및 로깅
  watchtower:
    image: containrrr/watchtower
    container_name: ollama-watchtower
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    environment:
      - WATCHTOWER_CLEANUP=true
      - WATCHTOWER_POLL_INTERVAL=3600  # 1시간마다 업데이트 확인
    restart: unless-stopped

volumes:
  ollama-data:
    driver: local

networks:
  default:
    driver: bridge